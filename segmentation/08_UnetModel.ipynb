{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a ResUnetA model\n",
    "\n",
    "Training of a `ResUnetA` architecture with the `.npz` files from the previous notebooks\n",
    "\n",
    "This notebook:\n",
    "\n",
    " * creates TensorFlow datasets using the npz files previously created. The datasets allow manipulation and loading on the fly, to reduce RAM load and processing of large AOIs\n",
    " * performs training of the k-fold models  \n",
    " * test the models predictions on a validation batch\n",
    " \n",
    "## NOTE\n",
    "\n",
    "This workflow can load the `.npz` files from the S3 bucket, or locally. In this case, we use data from disk (No S3 used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from typing import Callable, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from eoflow.models.metrics import MCCMetric\n",
    "from eoflow.models.segmentation_base import segmentation_metrics\n",
    "from eoflow.models.losses import JaccardDistanceLoss, TanimotoDistanceLoss\n",
    "\n",
    "from eoflow.models.segmentation_unets import ResUnetA\n",
    "\n",
    "# ### Changing current directory \n",
    "# os.chdir('/home/lscalambrin/proyecto_integrador/segmentation/field-delineation-main')\n",
    "# print(os.getcwd())\n",
    "\n",
    "from fd.tf_viz_utils import ExtentBoundDistVisualizationCallback\n",
    "from fd.training import TrainingConfig, get_dataset\n",
    "from fd.utils import prepare_filesystem\n",
    "from fd.metrics_extra import seg_metrics, mean_iou, mean_dice\n",
    "\n",
    "from pprint import pprint\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').disabled = True\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### paths\n",
    "save_patchlet_npz = '/data/lscalambrin/proyecto_integrador/segmentation/pergamino/patchlets_npz'\n",
    "df_path = '/data/lscalambrin/proyecto_integrador/segmentation/pergamino/patchlet-info.csv'\n",
    "idx_model = 1\n",
    "model_folder = f'/data/lscalambrin/proyecto_integrador/segmentation/pergamino/models/model{idx_model}'\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 5\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    bucket_name='bucket-name',\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key='',\n",
    "    aws_region='eu-central-1',\n",
    "    wandb_id=None, # change this with your wandb account \n",
    "    npz_folder=save_patchlet_npz,\n",
    "    metadata_path=df_path,\n",
    "    model_folder=model_folder,\n",
    "    model_s3_folder='models/Castilla/2020-04',\n",
    "    chkpt_folder=None,\n",
    "#     chkpt_folder='/home/ubuntu/pre-trained-model/checkpoints',\n",
    "    input_shape=(256, 256, 4),\n",
    "    n_classes=n_classes,\n",
    "    batch_size=batch_size,\n",
    "    iterations_per_epoch=n_samples//batch_size, \n",
    "    num_epochs=20,\n",
    "    model_name='resunet-a',\n",
    "    reference_names=['extent','boundary','distance'],\n",
    "    augmentations_feature=['flip_left_right', 'flip_up_down', 'rotate', 'brightness'],\n",
    "    augmentations_label=['flip_left_right', 'flip_up_down', 'rotate'],\n",
    "#   normalize posibles values:  'to_meanstd', 'to_medianstd', 'to_perc'\n",
    "    normalize='to_meanstd',\n",
    "    n_folds=2,\n",
    "    model_config={\n",
    "        'learning_rate': 0.0001,\n",
    "        'n_layers': 3,\n",
    "        'n_classes': n_classes,\n",
    "        'keep_prob': 0.8,\n",
    "        'features_root': 32,\n",
    "        'conv_size': 3,\n",
    "        'conv_stride': 1,\n",
    "        'dilation_rate': [1, 3, 15, 31],\n",
    "        'deconv_size': 2,\n",
    "        'add_dropout': True,\n",
    "        'add_batch_norm': False,\n",
    "        'use_bias': False,\n",
    "        'bias_init': 0.0,\n",
    "        'padding': 'SAME',\n",
    "        'pool_size': 3,\n",
    "        'pool_stride': 2,\n",
    "        'prediction_visualization': True,\n",
    "        'class_weights': None\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save model info\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "\n",
    "with open(model_folder +'/model_info.txt', 'w') as f:\n",
    "    sys.stdout = f # Change the standard output to the file we created.\n",
    "    pprint(vars(training_config))\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_config.wandb_id is not None:\n",
    "    !wandb login {training_config.wandb_id}  # EOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_folds = [get_dataset(training_config, fold=fold, augment=True, randomize=True,\n",
    "                        num_parallel=200, npz_from_s3=False) \n",
    "            for fold in range(1, training_config.n_folds+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fold_ex = ds_folds[0].batch(training_config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = next(iter(ds_fold_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = example_batch[0]\n",
    "lbls = example_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([5, 256, 256, 4]),\n",
       " TensorShape([5, 256, 256, 2]),\n",
       " TensorShape([5, 256, 256, 2]),\n",
       " TensorShape([5, 256, 256, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats['features'].shape, lbls['extent'].shape, lbls['boundary'].shape, lbls['distance'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=4, sharex='all', sharey='all', figsize=(20, 15))\n",
    "\n",
    "for nb in np.arange(3):\n",
    "    axs[nb][0].imshow(feats['features'].numpy()[nb][...,[2,1,0]])\n",
    "    axs[nb][1].imshow(lbls['extent'].numpy()[nb][..., 1])\n",
    "    axs[nb][2].imshow(lbls['boundary'].numpy()[nb][..., 1])\n",
    "    axs[nb][3].imshow(lbls['distance'].numpy()[nb][..., 1])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model & Train \n",
    "The following code sets up training of `n_folds` models, for a more robust implementation. If k-fold validation is not desired, the training dataset can be build by joining all dataset folds together, and training a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(config: TrainingConfig, chkpt_folder: str = None):\n",
    "    \"\"\" Initialise ResUnetA model \n",
    "    \n",
    "    If an existing chekpoints directory is provided, the existing weights are loaded and \n",
    "    training starts from existing state\n",
    "    \"\"\"\n",
    "    mcc_metric = MCCMetric(default_n_classes=n_classes, default_threshold=.5)\n",
    "    mcc_metric.init_from_config({'n_classes': n_classes})\n",
    "    \n",
    "    model = ResUnetA(training_config.model_config)\n",
    "    \n",
    "    model.build(dict(features=[None] + list(training_config.input_shape)))\n",
    "    \n",
    "    model.net.compile(\n",
    "        loss={'extent':TanimotoDistanceLoss(from_logits=False),\n",
    "              'boundary':TanimotoDistanceLoss(from_logits=False),\n",
    "              'distance':TanimotoDistanceLoss(from_logits=False)},\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=training_config.model_config['learning_rate']),\n",
    "        # comment out the metrics you don't care about\n",
    "        metrics=[segmentation_metrics['accuracy'](),\n",
    "#                  tf.keras.metrics.MeanIoU(num_classes=training_config.n_classes),\n",
    "                 mean_iou, mean_dice]\n",
    "    )\n",
    "    \n",
    "    if chkpt_folder is not None:\n",
    "        model.net.load_weights(f'{chkpt_folder}/model.ckpt')\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def initialise_callbacks(config: TrainingConfig, \n",
    "                         fold: int) -> Tuple[str, List[Callable]]:\n",
    "    \"\"\" Initialise callbacks used for logging and saving of models \"\"\"\n",
    "    now = datetime.now().isoformat(sep='-', timespec='seconds').replace(':', '-')\n",
    "    model_path = f'{training_config.model_folder}/{training_config.model_name}_fold-{fold}_{now}'\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    logs_path = os.path.join(model_path, 'logs')\n",
    "    checkpoints_path = os.path.join(model_path, 'checkpoints', 'model.ckpt')\n",
    "\n",
    "\n",
    "    # Tensorboard callback\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_path,\n",
    "                                                          update_freq='epoch',\n",
    "                                                          profile_batch=0)\n",
    "\n",
    "    # Checkpoint saving callback\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoints_path,\n",
    "                                                             save_best_only=True,\n",
    "                                                             save_freq='epoch',\n",
    "                                                             save_weights_only=True)\n",
    "\n",
    "    full_config = dict(**training_config.model_config, \n",
    "                       iterations_per_epoch=training_config.iterations_per_epoch, \n",
    "                       num_epochs=training_config.num_epochs, \n",
    "                       batch_size=training_config.batch_size,\n",
    "                       model_name=f'{training_config.model_name}_{now}'\n",
    "                      )\n",
    "\n",
    "    # Save model config \n",
    "    with open(f'{model_path}/model_cfg.json', 'w') as jfile:\n",
    "        json.dump(training_config.model_config, jfile)\n",
    "\n",
    "    # initialise wandb if used\n",
    "    if training_config.wandb_id:\n",
    "        wandb.init(config=full_config, \n",
    "                   name=f'{training_config.model_name}-leftoutfold-{fold}',\n",
    "                   project=\"field-delineation\", \n",
    "                   sync_tensorboard=True)\n",
    "        \n",
    "    callbacks = [tensorboard_callback, \n",
    "                 checkpoint_callback, \n",
    "                ] + ([WandbCallback()] if training_config.wandb_id is not None else [])\n",
    "    \n",
    "    return model_path, callbacks \n",
    "\n",
    "def plot_epochs(training_config, h, testing_id):\n",
    "    now = datetime.now().isoformat(sep='-', timespec='seconds').replace(':', '-')\n",
    "    model_path = f'{training_config.model_folder}/{training_config.model_name}_plots'\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "    np.save(model_path +  f'/data_train_{testing_id[0]+1}.npy',h)\n",
    "    \n",
    "    epochs = training_config.num_epochs\n",
    "#     tmp = np.load('data_train_1.npy',allow_pickle=True).item()\n",
    "    tmp = h\n",
    "    epoch = np.arange(epochs)+1\n",
    "\n",
    "    loss = tmp['loss']\n",
    "    val_loss = tmp['val_loss']\n",
    "\n",
    "    extent_loss = tmp['extent_loss']\n",
    "    val_extent_loss = tmp['val_extent_loss']\n",
    "    boundary_loss = tmp['boundary_loss']\n",
    "    val_boundary_loss = tmp['val_boundary_loss']\n",
    "    distance_loss = tmp['distance_loss']\n",
    "    val_distance_loss = tmp['val_distance_loss']\n",
    "\n",
    "    extent_accuracy = tmp['extent_accuracy']\n",
    "    val_extent_accuracy = tmp['val_extent_accuracy']\n",
    "    boundary_accuracy = tmp['boundary_accuracy']\n",
    "    val_boundary_accuracy = tmp['val_boundary_accuracy']\n",
    "    distance_accuracy = tmp['distance_accuracy']\n",
    "    val_distance_accuracy = tmp['val_distance_accuracy']\n",
    "    \n",
    "    ### loss\n",
    "    fig = plt.figure(figsize=[10, 5])\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(epoch,extent_loss, linewidth=2, marker='o',c='#7f7f7f')\n",
    "    ax.plot(epoch,boundary_loss, linewidth=2, marker='o',c='#17becf')\n",
    "    ax.plot(epoch,distance_loss, linewidth=2, marker='o',c='#9467bd')\n",
    "\n",
    "    ax.plot(epoch,val_extent_loss, linewidth=2, linestyle='dashed',c='#7f7f7f')\n",
    "    ax.plot(epoch,val_boundary_loss, linewidth=2, linestyle='dashed',c='#17becf')\n",
    "    ax.plot(epoch,val_distance_loss, linewidth=2, linestyle='dashed',c='#9467bd')\n",
    "    ax.legend(['Extent loss (training set)',\n",
    "               'Boundary loss (training set)',\n",
    "               'Distance loss (training set)',\n",
    "               'Validation set'],fontsize=14,shadow=True)\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Epoch', fontsize=16)\n",
    "    ax.set_ylabel('Loss function', fontsize=16)\n",
    "    # plt.title('Alexnet(edit) - CCE',fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.savefig(model_path + f'/loss_{testing_id[0]+1}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    ### acc\n",
    "    fig = plt.figure(figsize=[10, 5])\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(epoch,extent_accuracy, linewidth=2, marker='o',c='#7f7f7f')\n",
    "    ax.plot(epoch,boundary_accuracy, linewidth=2, marker='o',c='#17becf')\n",
    "    ax.plot(epoch,distance_accuracy, linewidth=2, marker='o',c='#9467bd')\n",
    "\n",
    "    ax.plot(epoch,val_extent_accuracy, linewidth=2, linestyle='dashed',c='#7f7f7f')\n",
    "    ax.plot(epoch,val_boundary_accuracy, linewidth=2, linestyle='dashed',c='#17becf')\n",
    "    ax.plot(epoch,val_distance_accuracy, linewidth=2, linestyle='dashed',c='#9467bd')\n",
    "    ax.legend(['Extent (training set)',\n",
    "               'Boundary (training set)',\n",
    "               'Distance (training set)',\n",
    "               'Validation set'],fontsize=14,shadow=True)\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Epoch', fontsize=16)\n",
    "    ax.set_ylabel('Accuracy', fontsize=16)\n",
    "    # plt.title('Alexnet(edit) - CCE',fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.savefig(model_path + f'/acc_{testing_id[0]+1}.pdf')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indices defining which dataset folds to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(range(training_config.n_folds))\n",
    "\n",
    "folds_ids_list = [(folds[:nf] + folds[1 + nf:], [nf]) for nf in folds]\n",
    "\n",
    "folds_ids_list = [folds_ids_list[1]]\n",
    "\n",
    "folds_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_ids, testing_id in folds_ids_list:\n",
    "    print(training_ids)\n",
    "    print(testing_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(training_config.seed)\n",
    "\n",
    "models = []\n",
    "model_paths = []\n",
    "\n",
    "for training_ids, testing_id in folds_ids_list:\n",
    "    \n",
    "    left_out_fold = testing_id[0]+1\n",
    "    print(f'Training model for left-out fold {left_out_fold}')\n",
    "    \n",
    "    ### No k-cross validation\n",
    "    fold_val = testing_id[0]\n",
    "    folds_train = training_ids\n",
    "    print(f'Train folds {folds_train}, Val fold: {fold_val}, Test fold: {testing_id[0]}')\n",
    "\n",
    "# bug fixed using tf.data.Dataset.concatenate\n",
    "#     ds_train = ds_folds[folds_train[0]]\n",
    "#     for ft in folds_train[1:]:\n",
    "#         ds_train.concatenate(ds_folds[ft])\n",
    "    ds_folds_train = [ds_folds[tid] for tid in folds_train]\n",
    "    ds_train = reduce(tf.data.Dataset.concatenate, ds_folds_train)\n",
    "    \n",
    "    \n",
    "    ds_val = ds_folds[fold_val]\n",
    "    \n",
    "    ds_val = ds_val.batch(training_config.batch_size)\n",
    "    \n",
    "    ds_train = ds_train.batch(training_config.batch_size)\n",
    "    ds_train = ds_train.repeat()\n",
    "    \n",
    "    print(type(ds_train))\n",
    "    \n",
    "    # Get model\n",
    "    model = initialise_model(training_config, chkpt_folder=training_config.chkpt_folder)\n",
    "    \n",
    "    # Set up callbacks to monitor training\n",
    "    model_path, callbacks = initialise_callbacks(training_config, \n",
    "                                                 fold=left_out_fold)\n",
    "    \n",
    "    print(f'\\tTraining model, writing to {model_path}')\n",
    "    \n",
    "\n",
    "    hist = model.net.fit(ds_train, \n",
    "                  validation_data=ds_val,\n",
    "                  epochs=training_config.num_epochs,\n",
    "                  steps_per_epoch=training_config.iterations_per_epoch,\n",
    "                  callbacks=callbacks, verbose=1)\n",
    "    \n",
    "    \n",
    "    plot_epochs(training_config,hist.history,testing_id)\n",
    "    \n",
    "    models.append(model)\n",
    "    model_paths.append(model_path)\n",
    "    \n",
    "    del fold_val, folds_train, ds_train, ds_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(ds_folds[1].batch(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.net.predict(test_batch[0]['features'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 3\n",
    "\n",
    "fig, axs = plt.subplots(nrows=n_images, ncols=5, \n",
    "                        sharex='all', sharey='all', \n",
    "                        figsize=(15, 3*n_images))\n",
    "\n",
    "for nb in np.arange(n_images):\n",
    "    axs[nb][0].imshow(test_batch[0]['features'].numpy()[nb][...,[2,1,0]])\n",
    "    axs[nb][1].imshow(predictions[0][nb][..., 1])\n",
    "    axs[nb][2].imshow(predictions[1][nb][..., 1])\n",
    "    axs[nb][3].imshow(predictions[2][nb][..., 1])\n",
    "    axs[nb][4].imshow(test_batch[1]['extent'].numpy()[nb][..., 1])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate models on test dataset\n",
    "\n",
    "Once we are happy with the hyper-parameters, we can test the performance of the models on the left-out test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: bear in mind that this score is computed on augmented samples, for a better score estimation recreate the datasets without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, testing_id in folds_ids_list:\n",
    "    \n",
    "    left_out_fold = testing_id[0]+1\n",
    "    print(f'Evaluating model on left-out fold {left_out_fold}')\n",
    "    \n",
    "    model.net.evaluate(ds_folds[testing_id[0]].batch(training_config.batch_size))\n",
    "    \n",
    "    print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
